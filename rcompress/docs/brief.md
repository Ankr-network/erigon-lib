
# Brief explonation of compression and decompression steps.


*Note: Since it was hard for me to understand the existing compressor, I used my previous experience writing compressor using ideas from [DEFLATE](https://www.w3.org/Graphics/PNG/RFC-1951) to write this compressor, so the files generated by existing compressor can not be used by this decompressor and vice versa. This compressor haven't been tested on real world data yet.*   


## Contents
- [Compressed file structure](#compressed-file-structure)
- [Creation of a dictionary](#creation-of-a-dictionary)
- [Dictionary compression and decompression](#dictionary-compression-and-decompression)
- [Block compression and decompression](#block-compression-and-decompression)
- [Possible improvements](#possible-improvements)


## Compressed file structure

![Compressed file structure](/rcompress/docs/compressed_file_structure.png)

1. **Header** contains information such as:
    1. Words count
    2. Empty words count
    3. Number of compressed blocks
    4. Maximum size of an uncompressed word (used to allocate maximum passible buffers for decoding)
    5. Size of the compressed dictionary

2. **Compressed Dictionary** is compressed static dictinary which is decompressed back to its uncompressed state when decompessor is created. Dictionary compression and decompression steps are briefly explained in [Dictionary compression and decompression](#dictionary-compression-and-decompression). For dictionary creation see [Creation of a dictionary](#creation-of-a-dictionary)

3. **Compressed Block** is compressed words whos total uncompressed size is less then or equal to 16MB. Dictionary compression and decompression steps are briefly explained in [Block compression and decompression](#block-compression-and-decompression)


## Creation of a dictionary

*TODO: write about `AddWord` method and uncompressed file in case someone new will read this.*

**Steps**:
1. Each worker processes a given byte array (array of unseparted words). In this process byte array is fed to the suffix array algorithm whos output is used to create LCP array. After finding common prefixes in byte array those common prefixes then inserted into the trie data structure for fast retrieval and to avoid large memory usage. Trie implemented using AVL tree. Each BST node in AVL tree takes exactly 40 byte (on 64-bit system). There is a limit on BST nodes `LIMIT_NODES = 16777216`, although this number never reached on tests. 

2. When all trie insertions are done, static dictionary is created using all prefixes in the trie.
This is recursive process. In result static dictionary will contain vector of prefixes of `size >= 4` and `size <= 255`. 

3. Creates blocks of words whos size <= 16MB, by reading word by word from uncomressed file and sends those blocks down the channel to precompress (or better to say create flags for compression, will be explained in [Block compression and decompression](#block-compression-and-decompression)). This will record flags in separate file. Every worker that does "precompress" after finishing its job passes (sends through the channel) the same block to another read-only function, whos job is to count number of prefixes that were used, and to separate prefixes with size 4 from the rest. 

4. Reduces dicitonary size and sorts it. Since some prefixes in dictionary may be refered once, it doesn't make sense to keep them in dictionary, so they are removed. Alos, removed prefixes with size 4, if refernce count is <= 2. Then all of the prefixes that hasn't been removed are sorted in descending order. So that the most referred prefix is in 0th position, next most referred is in 1st and so on. This is the final shape of the dictionary.


## Dictionary compression and decompression

Dictionary compression and decompression is similar to block compression and decompression. They share the same encoding and decoding techniques. The main differences are: 
- during dictionary compression dictionary encoder refers to the previously seen similar patterns in dictionary (LZ77 approach), while block compression/decompression is referred to dictionary for similar patterns. 
- dictionary decompression decodes entire dictionary at once, while block decompression decodes word by word.


*TODO: write this part more detailed.*

Dictionary compression/decompression is combination of Huffman coding and LZ77 with window size of 32KB. Previously seen patterns encoded as length of repeated pattern `match_len` and backward distance `back_ref`. 

`match_len` codes and literal bytes are combined into single alphabet. `match_len` and literal alphabet size is 284: 0..255 - literals, 256 - end of a word/prefix, 257..283 - `match_len` codes. All of these codes counted then Huffman codes generated (Huffman code max size 15 bit) and then dicitonary is encoded using those Huffman codes.

`back_ref` codes alphabet size is 30, each code is fixed size of 5 bits with possible extra bits, max extra bits <= 13

## Block compression and decompression

*TODO: write about precompress flags.*

Block compression and decompression is similar to dictionary compression and decompression. Their differences explained in [Dictionary compression and decompression](#dictionary-compression-and-decompression). 

In block compression instead of `back_ref` (as reference back to itself) it refers to the dictionary prefix - `prefix_id` and uses different alphabet for encoding/decoding. 

Block compression/decompression is combination of Huffman coding and references to the dictionary created by finding common patterns in a source. Common patterns encoded as `match_len` and `prefix_id` as reference to the dictionary index. 

`match_len` codes and literal bytes are combined into single alphabet and is the same as in dictionary encoding.

`prefix_id` codes alphabet size is 32, each code is fixed size of 5 bits with possible extra bits, max extra bits <= 19

Each word in a compressed block starts from "frash" byte, meaning that ending of one word and starting of new word do not share the same byte.

## Possible improvements

**Priority improvements**

1. Block decompressor. Currently block decompressor does both first it reads bit codes and then decodes those codes. This could be done in 2 workers, first worker reads bit codes sends them to the second, whos job is to restore those codes into original symbols. This can done by implementing somehing like golang channels in C++ or using queues or something else.

2. Dictionary creation and dictionary refernces. While testing this compressor, diffrent configurations were showing different results. So this part could be improved when testing on real world data. 

